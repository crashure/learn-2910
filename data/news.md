** post course testing
and more

**Session 15** *(Fri May 26, 10:00)*  

Don't forget to submit your presentations to the session 15 dropbox :)

I will be posting (on Slack or D2L) a self-criqitue worksheet for each
team to complete today, and there will be dropbox for it.

We will put some time aside this afternoon for you to complete course
evaluations too.

**Session 14** *(Fri May 26, 02:00)*  

I have gone through the survey results and bug reports, and found some fascinating results:

- Mustafar didn't participate, yet still got 27 ratings and an average score of 3.3/5?
Say what?  
I have discounted the bug reports for them too.
- A number of groups had glowing self-evaluations. Tsk, tsk.  
Those have triggered appropriate discounts.
- Given the crashes, with only two apps "surviving", I am astounded by the high reliability ratings...
an average of 3.4/5  
Mind you, some of the crashes were caused by customers inspecting the code and
figuring out how to crash the apps, and no one in the real world would do that, right?
- I will share the results, but I think it is fair to say that you have very different
expectations and interpretations than we do!


**Bug Bounties**  

- I have ignored the Mustafar bug reports - they did not participate.  
- There were some bug reports with no app/team identified - ignored. 
- there were a number of "database crashed" or "database crashed every time"; a bug report
needs to identify what you were doing when this happened; this was explained
in the session writeup.
- self-reported bugs not appropriate
- 27 bugs awarded, paid for at 0.5 marks each, out of the session 14 grades.

**Clarification**

There were 53 "bugs" reported. A number of those were not helpful, unclear, or
not deemed a "bug". Others were duplicates, and the first to report a bug was awarded
the bounty for it.

I forgot to put the spreadsheet with all of the reports onto my laptop, but have it here at my fingertips
if you have any questions!
